"""
evaluation/evaluate_model.py
============================
‡∏™‡∏Ñ‡∏£‡∏¥‡∏õ‡∏ï‡πå‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏á‡∏≤‡∏ô‡∏ß‡∏¥‡∏à‡∏±‡∏¢ Crypto AI

‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡∏î‡∏±‡∏á‡∏ô‡∏µ‡πâ:
1. ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏ù‡∏∂‡∏Å‡πÅ‡∏•‡πâ‡∏ß (LSTM, GRU) ‡πÅ‡∏•‡∏∞ Scalers
2. ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡∏¥‡∏ö‡πÅ‡∏•‡∏∞‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° Test Set (Multi-Feature)
3. ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ö‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÄ‡∏Ñ‡∏¢‡πÄ‡∏´‡πá‡∏ô (Test Set)
4. ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Baseline Models (Naive Forecast, Moving Average)
5. ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏•‡∏≠‡∏á (‡∏Ñ‡πà‡∏≤‡∏à‡∏£‡∏¥‡∏á vs ‡∏Ñ‡πà‡∏≤‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢) ‡πÑ‡∏õ‡∏¢‡∏±‡∏á experiments/
6. ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏ï‡∏±‡∏ß‡∏ä‡∏µ‡πâ‡∏ß‡∏±‡∏î‡∏ó‡∏≤‡∏á‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥ (MAE, RMSE, MAPE, Directional Accuracy)
"""

import os
import sys
import pandas as pd
import joblib
import numpy as np
from tensorflow.keras.models import load_model

# ‡πÄ‡∏û‡∏¥‡πà‡∏° project root ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from train import config
from train.preprocessing import DataProcessor
from evaluation.metrics import (
    calculate_mae, calculate_rmse, calculate_mape, 
    calculate_directional_accuracy
)
from utils.scaling import inverse_transform_close

# ---------------------------------------------------------
# ‡∏Ñ‡πà‡∏≤‡∏Ñ‡∏á‡∏ó‡∏µ‡πà (CONSTANTS)
# ---------------------------------------------------------
REPORT_FILE = os.path.join(config.RESULT_DIR, "evaluation_report.csv")


def compute_baseline_predictions(actual_series):
    """
    ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Baseline Models ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Å‡∏±‡∏ö Deep Learning
    
    Baselines:
    1. Naive Forecast: ‡πÉ‡∏ä‡πâ‡∏£‡∏≤‡∏Ñ‡∏≤‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡πà‡∏≤‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢ (y_pred[t] = y_actual[t-1])
    2. Moving Average (7): ‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡πÄ‡∏Ñ‡∏•‡∏∑‡πà‡∏≠‡∏ô‡∏ó‡∏µ‡πà 7 ‡∏ä‡πà‡∏ß‡∏á‡πÄ‡∏ß‡∏•‡∏≤
    
    Args:
        actual_series: np.array ‡∏Ç‡∏≠‡∏á‡∏£‡∏≤‡∏Ñ‡∏≤‡∏à‡∏£‡∏¥‡∏á (inverse-transformed)
        
    Returns:
        dict: {
            "naive": {"predictions": np.array, "start_idx": int},
            "ma7": {"predictions": np.array, "start_idx": int}
        }
    """
    actual = np.array(actual_series).flatten()
    
    # 1. Naive Forecast: y_pred[t] = y_actual[t-1]
    # ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏à‡∏≤‡∏Å index 1 ‡πÄ‡∏û‡∏£‡∏≤‡∏∞ Naive ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏Ñ‡πà‡∏≤‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤ 1 ‡∏ï‡∏±‡∏ß
    naive_preds = actual[:-1]  # ‡∏Ñ‡πà‡∏≤‡∏à‡∏£‡∏¥‡∏á‡∏Ç‡∏≠‡∏á step ‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤
    
    # 2. Moving Average (7): ‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢ 7 ‡∏ä‡πà‡∏ß‡∏á‡πÄ‡∏ß‡∏•‡∏≤‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤
    ma_period = 7
    ma_preds = []
    for i in range(ma_period, len(actual)):
        ma_preds.append(np.mean(actual[i - ma_period:i]))
    ma_preds = np.array(ma_preds)
    
    return {
        "naive": {"predictions": naive_preds, "start_idx": 1},
        "ma7": {"predictions": ma_preds, "start_idx": ma_period}
    }


def evaluate_models():
    """
    ‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏•‡∏´‡∏•‡∏±‡∏Å (Main Evaluation Workflow)
    ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö Multi-Feature Models + Baseline Comparison + Directional Accuracy
    """
    config._ensure_directories(config.RESULT_DIR)
    
    summary_results = []
    
    models_to_eval = [
        {"type": "LSTM", "prefix": "lstm"},
        {"type": "GRU", "prefix": "gru"}
    ]

    print("=" * 60)
    print(" STARTING RESEARCH EVALUATION (with Baselines)")
    print("=" * 60)

    for coin in config.COINS:
        for tf in config.TIMEFRAMES:
            print(f"\nProcessing Dataset: {coin.upper()} ({tf})")
            
            # 1. ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (Load Data) - Multi-Feature
            processor = DataProcessor()
            try:
                df_raw = processor.load_data(coin, tf)
                timestamps = df_raw['timestamp'].values
                
                # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (Scaling) - Multi-Feature
                train_scaled, test_scaled, scaler = processor.get_train_test_data(df_raw)
                
                # ‡∏™‡∏£‡πâ‡∏≤‡∏á Sliding Window ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Test Set
                X_test, y_test = processor.create_sliding_window(test_scaled)
                
                # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Timestamps ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Test Set
                train_size = len(train_scaled)
                test_start_index = train_size + config.WINDOW_SIZE
                test_timestamps = timestamps[test_start_index:]
                
                if len(test_timestamps) != len(y_test):
                    test_timestamps = test_timestamps[:len(y_test)]

            except Exception as e:
                print(f"   ‚ùå Error loading/processing data: {e}")
                import traceback
                traceback.print_exc()
                continue
            
            # ‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡πÄ‡∏Å‡πá‡∏ö actual_series ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö baseline (‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß)
            actual_series_for_baseline = None
            
            # 2. ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏•‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÇ‡∏°‡πÄ‡∏î‡∏• (LSTM, GRU)
            for m in models_to_eval:
                model_type = m["type"]
                prefix = m["prefix"]
                
                model_filename = f"{prefix}_{coin.lower()}_{tf}.h5"
                scaler_filename = f"scaler_{coin.lower()}_{tf}.save"
                
                model_path = os.path.join(config.MODEL_DIR, model_filename)
                scaler_path = os.path.join(config.MODEL_DIR, scaler_filename)
                
                if not os.path.exists(model_path):
                    print(f"   ‚ö†Ô∏è  Model not found: {model_filename}")
                    continue
                    
                try:
                    model = load_model(model_path, compile=False)
                    saved_scaler = joblib.load(scaler_path)
                    
                    print(f"    Evaluating {model_type}...")
                    preds_scaled = model.predict(X_test, verbose=0)
                    
                    # Inverse transform (using shared utility)
                    preds_actual = inverse_transform_close(saved_scaler, preds_scaled)
                    actual_series = inverse_transform_close(saved_scaler, y_test)
                    
                    preds_series = preds_actual.flatten()
                    actual_series = actual_series.flatten()
                    
                    # ‡πÄ‡∏Å‡πá‡∏ö actual_series ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö baseline (‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÅ‡∏£‡∏Å‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô)
                    if actual_series_for_baseline is None:
                        actual_series_for_baseline = actual_series.copy()
                    
                    # 3. ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏•‡∏≠‡∏á
                    predictions_dir = os.path.join(config.RESULT_DIR, "predictions")
                    os.makedirs(predictions_dir, exist_ok=True)
                    
                    experiment_filename = f"{coin.lower()}_{model_type.lower()}_{tf}_predictions.csv"
                    experiment_path = os.path.join(predictions_dir, experiment_filename)
                    
                    df_experiment = pd.DataFrame({
                        "timestamp": test_timestamps,
                        "actual": actual_series,
                        "predicted": preds_series
                    })
                    
                    df_experiment.to_csv(experiment_path, index=False)
                    print(f"       Saved predictions: {experiment_path}")
                    
                    # 4. ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏ï‡∏±‡∏ß‡∏ä‡∏µ‡πâ‡∏ß‡∏±‡∏î (using shared metrics + directional accuracy)
                    mae = calculate_mae(actual_series, preds_series)
                    rmse = calculate_rmse(actual_series, preds_series)
                    mape = calculate_mape(actual_series, preds_series)
                    da = calculate_directional_accuracy(actual_series, preds_series)
                    
                    summary_results.append({
                        "model": model_type,
                        "timeframe": tf,
                        "coin": coin.upper(),
                        "mae": round(mae, 4),
                        "rmse": round(rmse, 4),
                        "mape": round(mape, 4),
                        "da": round(da, 2)
                    })
                    
                    print(f"       MAE: {mae:.2f}, RMSE: {rmse:.2f}, MAPE: {mape:.2f}%, DA: {da:.2f}%")
                    
                except Exception as e:
                    print(f"      ‚ùå Evaluation failed: {e}")
                    import traceback
                    traceback.print_exc()
            
            # 3. ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Baseline Models (Naive + MA7)
            if actual_series_for_baseline is not None and len(actual_series_for_baseline) > 7:
                print(f"\n    üìê Computing Baselines for {coin.upper()} ({tf})...")
                baselines = compute_baseline_predictions(actual_series_for_baseline)
                
                for baseline_name, baseline_data in baselines.items():
                    start_idx = baseline_data["start_idx"]
                    b_preds = baseline_data["predictions"]
                    b_actual = actual_series_for_baseline[start_idx:]
                    
                    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ô
                    min_len = min(len(b_preds), len(b_actual))
                    b_preds = b_preds[:min_len]
                    b_actual = b_actual[:min_len]
                    
                    if min_len == 0:
                        continue
                    
                    mae = calculate_mae(b_actual, b_preds)
                    rmse = calculate_rmse(b_actual, b_preds)
                    mape = calculate_mape(b_actual, b_preds)
                    da = calculate_directional_accuracy(b_actual, b_preds)
                    
                    display_name = "Naive" if baseline_name == "naive" else "MA(7)"
                    
                    summary_results.append({
                        "model": display_name,
                        "timeframe": tf,
                        "coin": coin.upper(),
                        "mae": round(mae, 4),
                        "rmse": round(rmse, 4),
                        "mape": round(mape, 4),
                        "da": round(da, 2)
                    })
                    
                    print(f"       {display_name}: MAE={mae:.2f}, RMSE={rmse:.2f}, MAPE={mape:.2f}%, DA={da:.2f}%")

    # 5. ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏™‡∏£‡∏∏‡∏õ
    if summary_results:
        df_summary = pd.DataFrame(summary_results)
        df_summary.to_csv(REPORT_FILE, index=False)
        
        print("\n" + "=" * 60)
        print(" EVALUATION SUMMARY (Saved to evaluation_report.csv)")
        print("=" * 60)
        print(df_summary.to_string(index=False))
        
        # ‡πÅ‡∏¢‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå DL models ‡∏Å‡∏±‡∏ö baselines
        dl_models = df_summary[df_summary['model'].isin(['LSTM', 'GRU'])]
        baselines = df_summary[df_summary['model'].isin(['Naive', 'MA(7)'])]
        
        if not dl_models.empty:
            best_model_idx = dl_models['mape'].idxmin()
            best_model = dl_models.loc[best_model_idx]
            print("\nüèÜ BEST PERFORMING MODEL:")
            print(f"   {best_model['model']} on {best_model['coin']} ({best_model['timeframe']})")
            print(f"   MAPE: {best_model['mape']}% | DA: {best_model['da']}%")
        
        if not baselines.empty:
            print("\nüìê BASELINE COMPARISON:")
            for _, row in baselines.iterrows():
                print(f"   {row['model']} on {row['coin']} ({row['timeframe']}): "
                      f"MAPE={row['mape']}%, DA={row['da']}%")
        
        readme_path = os.path.join(config.RESULT_DIR, "README.md")
        with open(readme_path, "w", encoding="utf-8") as f:
            f.write("# ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏•‡∏≠‡∏á (Experimental Results)\n\n")
            f.write("‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ô‡∏µ‡πâ‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏î‡∏¥‡∏ö (Raw Prediction) ‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÇ‡∏°‡πÄ‡∏î‡∏•\n\n")
            f.write("## ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡πÑ‡∏ü‡∏•‡πå (File Format)\n")
            f.write("- **‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå**: `{coin}_{model}_{timeframe}_predictions.csv`\n")
            f.write("- **‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå**:\n")
            f.write("  - `timestamp`: ‡∏ß‡∏±‡∏ô‡πÅ‡∏•‡∏∞‡πÄ‡∏ß‡∏•‡∏≤‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•\n")
            f.write("  - `actual`: ‡∏£‡∏≤‡∏Ñ‡∏≤‡∏õ‡∏¥‡∏î‡∏à‡∏£‡∏¥‡∏á\n")
            f.write("  - `predicted`: ‡∏£‡∏≤‡∏Ñ‡∏≤‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÇ‡∏î‡∏¢‡πÇ‡∏°‡πÄ‡∏î‡∏• AI\n\n")
            f.write("## ‡∏ï‡∏±‡∏ß‡∏ä‡∏µ‡πâ‡∏ß‡∏±‡∏î‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ (Evaluation Metrics)\n")
            f.write("- **MAE**: Mean Absolute Error\n")
            f.write("- **RMSE**: Root Mean Squared Error\n")
            f.write("- **MAPE**: Mean Absolute Percentage Error (%)\n")
            f.write("- **DA**: Directional Accuracy (%) ‚Äî ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ó‡∏¥‡∏®‡∏ó‡∏≤‡∏á‡∏£‡∏≤‡∏Ñ‡∏≤\n\n")
            f.write("## Baseline Models\n")
            f.write("- **Naive Forecast**: ‡πÉ‡∏ä‡πâ‡∏£‡∏≤‡∏Ñ‡∏≤‡∏õ‡∏¥‡∏î‡∏Ç‡∏≠‡∏á‡πÅ‡∏ó‡πà‡∏á‡πÄ‡∏ó‡∏µ‡∏¢‡∏ô‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡πà‡∏≤‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢\n")
            f.write("- **MA(7)**: ‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡πÄ‡∏Ñ‡∏•‡∏∑‡πà‡∏≠‡∏ô‡∏ó‡∏µ‡πà 7 ‡∏ä‡πà‡∏ß‡∏á‡πÄ‡∏ß‡∏•‡∏≤‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤\n")
            
    else:
        print("\n‚ùå No models were evaluated successfully.")

if __name__ == "__main__":
    evaluate_models()

